# kafka_stream
![Flow2](https://github.com/user-attachments/assets/e83d471e-f691-48db-a270-2d08e286a32a)
A real-time data pipeline architecture using Spark Streaming, Kafka, Upstash, Databricks, and Delta Lake. The focus is on the flow of data from ingestion to real-time processing.

Data Ingestion:

Source: The pipeline begins with CSV files that are ingested using Spark Streaming.
Upstash and Kafka:
Upstash: A cloud-native database service optimized for serverless environments, used here for efficient message brokering and data streaming.
Kafka: Acts as the primary message broker, receiving data from Upstash. Kafka enables real-time streaming and decoupling of data producers and consumers.
Real-Time Processing:

Spark Streaming: Continues processing data from Kafka, ensuring the data is kept up-to-date and processed in real-time as it flows through the pipeline.
Data Storage and Transformation:

The processed data is stored in Databricks using Delta Lake, organized into three layers (Bronze, Silver, and Gold):

![Flow1](https://github.com/user-attachments/assets/e96cec11-a114-4166-91af-b26181a2f8af)
Illustration of data pipeline architecture leveraging Kafka, Delta Lake, and Databricks. The data processing is divided into three layers: Bronze, Silver, and Gold.

Bronze Layer:

Source: Data is ingested into the Bronze layer using Kafka. This layer stores raw, unprocessed data.
Data Flow: Data from Kafka is streamed into a Bronze table in Delta Lake.
Silver Layer:

Transformation: The raw data from the Bronze layer is cleaned and transformed into the Silver layer.
Tables: There are two Silver tables:
Stocks: Contains transformed stock data.
Close Price (SCD 2): Handles slowly changing dimensions (Type 2) for stock close prices. This table tracks changes over time.
Current Close: Extracts the most recent close price from the SCD 2 table.
Data Flow:
Batch processing is used to move data from the Bronze table to the Stocks table.
Streaming is used to update the Close Price (SCD 2) and Current Close tables.
Gold Layer:

Aggregated and Enriched Data: This layer contains the final, refined data ready for analysis.
Table:
Updated Stocks: Combines data from the Stocks and Current Close tables to provide a comprehensive view of the current stock status.
Data Flow: Batch processing is used to create the Updated Stocks table from the Stocks Close data.




The 'kafka_stream' project was generated by using the default-python template.

## CI/CD Implemention using DAB and Github Actions

1. Install the Databricks CLI from https://docs.databricks.com/dev-tools/cli/databricks-cli.html

2. Authenticate to your Databricks workspace, if you have not done so already:
    ```
    $ databricks configure
    ```

3. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] kafka_stream_job` to your workspace.
    You can find that job by opening your workpace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

   Note that the default job from the template has a schedule that runs every day
   (defined in resources/kafka_stream_job.yml). The schedule
   is paused when deploying in development mode (see
   https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).

5. To run a job or pipeline, use the "run" command:
   ```
   $ databricks bundle run
   ```

6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   https://docs.databricks.com/dev-tools/vscode-ext.html.

7. For documentation on the Databricks asset bundles format used
   for this project, and for CI/CD configuration, see
   https://docs.databricks.com/dev-tools/bundles/index.html.
